{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9hPbP053j7wy"
   },
   "source": [
    "# Natural Language Inference\n",
    "\n",
    "This Jupyter Notebook fine-tunes `microsoft/mdeberta-v3-base` on the `Contradictory, My Dear Watson` dataset.\n",
    "It includes data loading, preprocessing, data augmentation, model training, evaluation, and visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kb4jC0vEj7w0"
   },
   "source": [
    "### Environment setup\n",
    "\n",
    "We use a local server of Jupyter Notebook in a LXC connected to 4 GPU: A100 MXP 80GB to train the model.\n",
    "\n",
    "We use a custom kernel created from Python 3.11."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mqhGjxHoj7w0"
   },
   "outputs": [],
   "source": [
    "%pip install -qU pandas numpy seaborn matplotlib scikit-learn datasets nltk nlpaug\n",
    "%pip install -qU \"transformers[torch]\"\n",
    "# %pip install -qU protobuf tiktoken sentencepiece # depends on models used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VbTlBIhLj7w1"
   },
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tw4RAm1zj7w1"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset, concatenate_datasets, load_dataset\n",
    "\n",
    "# Data augmentation\n",
    "from nlpaug.augmenter.word import WordAugmenter, RandomWordAug, SynonymAug\n",
    "from nlpaug.augmenter.char import KeyboardAug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i8Bk9-ya-7EE"
   },
   "outputs": [],
   "source": [
    "ROOT_PATH = Path.cwd()\n",
    "# Use fixed seed for results reproducibility\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_U6tDwWAj7w1"
   },
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenge dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "roW0dM9Mj7w1"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(ROOT_PATH / \"data/train.csv\")\n",
    "df.drop(columns=[\"id\"], inplace=True)\n",
    "\n",
    "# Train-test spliting\n",
    "challenge_ds = Dataset.from_pandas(df).train_test_split(test_size=0.3, seed=0)\n",
    "challenge_train = challenge_ds['train']\n",
    "challenge_val = challenge_ds['test']\n",
    "# Inspect the dataset with 10 random row\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w4XYZOceWvJ4"
   },
   "source": [
    "Display label distribution: the dataset is evenly distributed across all labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iI0wu0VxWt7m"
   },
   "outputs": [],
   "source": [
    "df.label.value_counts().plot(kind=\"bar\", figsize=(10, 6), title=\"Pair-wise sentences distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m_JGmdbPXSxg"
   },
   "source": [
    "However, the dataset predominantly consists of English sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xJcuSwR9VIZm"
   },
   "outputs": [],
   "source": [
    "df.language.value_counts().plot(\n",
    "    kind=\"pie\", \n",
    "    figsize=(10, 10), \n",
    "    autopct='%1.1f%%', \n",
    "    title=\"Languages distribution\"\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MNLI dataset\n",
    "\n",
    "We use `matched` version of test and train set because the dataset from the Challenge seems to match the same genre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnli_train = load_dataset(\"nyu-mll/glue\", \"mnli\", split=\"train\")\n",
    "mnli_val = load_dataset(\"nyu-mll/glue\", \"mnli\", split=\"validation_matched\")\n",
    "\n",
    "# Inspect the dataset\n",
    "mnli_train_df = mnli_train.to_pandas()\n",
    "mnli_train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XNLI dataset\n",
    "\n",
    "[XNLI: Evaluating Cross-lingual Sentence Representations](https://aclanthology.org/D18-1269/) (Conneau et al., EMNLP 2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_xnli_datasets(languages: list, split=\"train\"):\n",
    "    datasets = []\n",
    "    for lang in languages:\n",
    "        xnli_lang = load_dataset(\"facebook/xnli\", lang, split=split)\n",
    "        xnli_lang = xnli_lang.add_column(\"lang_abv\", [lang] * len(xnli_lang))\n",
    "        datasets.append(xnli_lang)\n",
    "\n",
    "    return concatenate_datasets(datasets).shuffle(seed=0)\n",
    "\n",
    "languages = ['ar','bg','de','el','en','es','fr','hi','ru','sw','th','tr','ur','vi','zh']\n",
    "xnli_train = load_xnli_datasets(languages, split=\"train\")\n",
    "xnli_val = load_xnli_datasets(languages, split=\"validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we inspect the XNLI dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xnli_train_df = xnli_train.to_pandas()\n",
    "# xnli_train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-4Nby-Mvj7w2"
   },
   "source": [
    "### Data Augmentation\n",
    "\n",
    "Use `NLPAug` library to augment the data by using synonyms, typo insertion and word swapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_WPS4lljj7w2"
   },
   "outputs": [],
   "source": [
    "def augment_text(text, augmenter):\n",
    "    try:\n",
    "        result = augmenter.augment(text)\n",
    "        # Handle list outputs from some augmenters\n",
    "        return result[0] if isinstance(result, list) else str(result)\n",
    "    except Exception as e:\n",
    "        return str(text)  # Ensure string return\n",
    "\n",
    "def augment_df(df, augmenters, sample_frac=0.5):\n",
    "    sample = df.sample(frac=sample_frac)\n",
    "    augmented = []\n",
    "\n",
    "    for _, row in tqdm(sample.iterrows(), total=len(sample)):\n",
    "        for aug, weight in augmenters:\n",
    "            if np.random.random() > weight:\n",
    "                continue  # Skip this augmentation\n",
    "\n",
    "            new_row = row.copy()\n",
    "            premise = new_row['premise'] = augment_text(row['premise'], aug)\n",
    "            hypothesis = new_row['hypothesis'] = augment_text(row['hypothesis'], aug)\n",
    "            if premise != row[\"premise\"] or hypothesis != row[\"hypothesis\"]:\n",
    "                augmented.append(new_row)\n",
    "\n",
    "    return pd.DataFrame(augmented).convert_dtypes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KjQLFQq8T7QR"
   },
   "source": [
    "Define augmentation strategies and their probability to be used for each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lSLivR29BGwt"
   },
   "outputs": [],
   "source": [
    "augmenters = [\n",
    "    (SynonymAug(aug_src='wordnet', aug_p=0.1), 0.8),   # Synonym replacement\n",
    "    (RandomWordAug(action='swap', aug_p=0.1), 0.1),    # Word swapping\n",
    "    (KeyboardAug(aug_char_p=0.1, aug_word_p=0.1), 0.3) # Typo simulation\n",
    "]\n",
    "\n",
    "df_aug = augment_df(df, augmenters)\n",
    "print(f\"Adding {len(df_aug)} new examples\")\n",
    "\n",
    "# Concatenate augmented dataset with original\n",
    "challenge_aug = Dataset.from_pandas(df_aug)\n",
    "train_set = concatenate_datasets([challenge_train, challenge_aug]).shuffle()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZBykOxZDj7w3"
   },
   "source": [
    "### Model configuration\n",
    "\n",
    "**Architecture:**  \n",
    "- `microsoft/mdeberta-v3-base` transformer from HuggingFace\n",
    "- Classification head with dropout (0.3)\n",
    "\n",
    "**Training Parameters:**\n",
    "- Batch size: 8 (train), 8 (eval)\n",
    "- Learning rate: 2e-5\n",
    "- Epochs: 5\n",
    "- Weight decay: 0.01\n",
    "- Warmup ratio: 0.1\n",
    "\n",
    "Train using the `Trainer` API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jyv-9MoVJWy6"
   },
   "source": [
    "#### Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8o6XGXtgJhGB"
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = \"FacebookAI/xlm-roberta-large\"\n",
    "MODEL_BASENAME = MODEL_NAME.rpartition('/')[2]\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=3)\n",
    "model.classifier.dropout = torch.nn.Dropout(0.3)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)  # Move model to GPU if available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f-ix6gD6JJFR"
   },
   "source": [
    "#### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ts_F70Doj7w3"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def tokenize_function(row):\n",
    "    return tokenizer(row['premise'], row['hypothesis'], row[\"lang_abv\"], padding='longest')\n",
    "\n",
    "challenge_train_tokenized = challenge_train.map(tokenize_function, batched=True)\n",
    "challenge_val_tokenized = challenge_val.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VQrSbqSFT7QT"
   },
   "source": [
    "#### Training hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "peRqyrEqT7QT"
   },
   "outputs": [],
   "source": [
    "model_path = ROOT_PATH / \"models\" / f\"{MODEL_BASENAME}-mnli-xnli\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_path,\n",
    "    eval_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.1,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='accuracy',\n",
    "    report_to='none'  # Disabling wandb callbacks\n",
    ")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    y_pred = np.argmax(eval_pred.predictions, axis=1)\n",
    "    return dict(accuracy=accuracy_score(eval_pred.label_ids, y_pred))\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=challenge_train_tokenized,\n",
    "    eval_dataset=challenge_val_tokenized,\n",
    "    processing_class=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KkuxGob7j7w4"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jxQF4kis6StP"
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6aBONMs6Ltem"
   },
   "source": [
    "Save the best model and the tokenizer to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nA9nEKB7LwLl"
   },
   "outputs": [],
   "source": [
    "# Save the best model\n",
    "trainer.save_model(model_path)\n",
    "tokenizer.save_pretrained(model_path)\n",
    "print(f\"Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wl19KSO2K5PP"
   },
   "source": [
    "### Evaluation & Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qZ94KVS5LHkg"
   },
   "source": [
    "Compute accuracy on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OvK1-BMbj7w4"
   },
   "outputs": [],
   "source": [
    "pred = trainer.predict(test_set)\n",
    "y_true = test_set['label']\n",
    "y_pred = np.argmax(pred.predictions, axis=-1)\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f\"\\nAccuracy: {accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mNVClqebLOXF"
   },
   "source": [
    "Display the confusion matrix using `seaborn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_d1sOBH1LDcQ"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(confusion_matrix(y_true, y_pred),\n",
    "            annot=True,\n",
    "            fmt='d',\n",
    "            cmap='Blues',\n",
    "            xticklabels=['Entailment', 'Neutral', 'Contradiction'],\n",
    "            yticklabels=['Entailment', 'Neutral', 'Contradiction'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jF49SrkCLWO7"
   },
   "outputs": [],
   "source": [
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=['Entailment', 'Neutral', 'Contradiction']))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
