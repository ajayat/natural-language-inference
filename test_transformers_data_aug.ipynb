{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9hPbP053j7wy"
   },
   "source": [
    "# Natural Language Inference\n",
    "\n",
    "This Jupyter Notebook fine-tunes `microsoft/mdeberta-v3-base` on the `Contradictory, My Dear Watson` dataset.\n",
    "It includes data loading, preprocessing, data augmentation, model training, evaluation, and visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kb4jC0vEj7w0"
   },
   "source": [
    "### Environment Setup\n",
    "\n",
    "This notebook was executed on a local Jupyter server in a LXC connected to 4 GPU: A100 MXP 80GB to train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mqhGjxHoj7w0"
   },
   "outputs": [],
   "source": [
    "%pip install -qU pandas numpy seaborn matplotlib scikit-learn datasets nltk nlpaug\n",
    "%pip install -qU \"transformers[torch]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VbTlBIhLj7w1"
   },
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tw4RAm1zj7w1"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset, concatenate_datasets, load_dataset\n",
    "\n",
    "# Data augmentation\n",
    "from nlpaug.augmenter.word import RandomWordAug, SynonymAug\n",
    "from nlpaug.augmenter.char import KeyboardAug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i8Bk9-ya-7EE"
   },
   "outputs": [],
   "source": [
    "ROOT_PATH = Path.cwd()\n",
    "# Use fixed seed for results reproducibility\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_U6tDwWAj7w1"
   },
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenge dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "roW0dM9Mj7w1"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(ROOT_PATH / \"data/train.csv\")\n",
    "df.drop(columns=[\"id\"], inplace=True)\n",
    "\n",
    "# Train-test spliting\n",
    "challenge_dataset = Dataset.from_pandas(df).train_test_split(test_size=0.3, seed=0)\n",
    "challenge_train = challenge_dataset['train']\n",
    "challenge_val = challenge_dataset['test']\n",
    "# Inspect the dataset with 10 random row\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w4XYZOceWvJ4"
   },
   "source": [
    "Display label distribution: the dataset is evenly distributed across all labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iI0wu0VxWt7m"
   },
   "outputs": [],
   "source": [
    "df.label.value_counts().plot(kind=\"bar\", figsize=(10, 6), title=\"Pair-wise sentences distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m_JGmdbPXSxg"
   },
   "source": [
    "However, the dataset predominantly consists of English sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xJcuSwR9VIZm"
   },
   "outputs": [],
   "source": [
    "df.language.value_counts().plot(\n",
    "    kind=\"pie\", \n",
    "    figsize=(10, 10), \n",
    "    autopct='%1.1f%%', \n",
    "    title=\"Languages distribution\"\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MNLI dataset\n",
    "\n",
    "We use `matched` version of test and train set because the dataset from the Challenge seems to match the same genre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnli_train = load_dataset(\"nyu-mll/glue\", \"mnli\", split=\"train\")\n",
    "mnli_val = load_dataset(\"nyu-mll/glue\", \"mnli\", split=\"validation_matched\")\n",
    "\n",
    "# Inspect the dataset\n",
    "mnli_train_df = mnli_train.to_pandas()\n",
    "mnli_train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XNLI dataset\n",
    "\n",
    "[XNLI: Evaluating Cross-lingual Sentence Representations](https://aclanthology.org/D18-1269/) (Conneau et al., EMNLP 2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_xnli_datasets(languages: list, split=\"train\"):\n",
    "    datasets = []\n",
    "    for lang in languages:\n",
    "        xnli_lang = load_dataset(\"facebook/xnli\", lang, split=split)\n",
    "        xnli_lang = xnli_lang.add_column(\"lang_abv\", [lang] * len(xnli_lang))\n",
    "        datasets.append(xnli_lang)\n",
    "\n",
    "    return concatenate_datasets(datasets).shuffle(seed=0)\n",
    "\n",
    "languages = ['ar','bg','de','el','en','es','fr','hi','ru','sw','th','tr','ur','vi','zh']\n",
    "xnli_train = load_xnli_datasets(languages, split=\"train\")\n",
    "xnli_val = load_xnli_datasets(languages, split=\"validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we inspect the XNLI dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xnli_train_df = xnli_train.to_pandas()\n",
    "# xnli_train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-4Nby-Mvj7w2"
   },
   "source": [
    "### Data Augmentation\n",
    "\n",
    "Use `NLPAug` library to augment the data by using synonyms, typo insertion and word swapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_WPS4lljj7w2"
   },
   "outputs": [],
   "source": [
    "def augment_text(text, augmenter):\n",
    "    try:\n",
    "        result = augmenter.augment(text)\n",
    "        # Handle list outputs from some augmenters\n",
    "        return result[0] if isinstance(result, list) else str(result)\n",
    "    except Exception as e:\n",
    "        return str(text)  # Ensure string return\n",
    "\n",
    "def augment_df(df, augmenters, sample_frac=0.5):\n",
    "    sample = df.sample(frac=sample_frac)\n",
    "    augmented = []\n",
    "\n",
    "    for _, row in tqdm(sample.iterrows(), total=len(sample)):\n",
    "        for aug, weight in augmenters:\n",
    "            if np.random.random() > weight:\n",
    "                continue  # Skip this augmentation\n",
    "\n",
    "            new_row = row.copy()\n",
    "            premise = new_row['premise'] = augment_text(row['premise'], aug)\n",
    "            hypothesis = new_row['hypothesis'] = augment_text(row['hypothesis'], aug)\n",
    "            if premise != row[\"premise\"] or hypothesis != row[\"hypothesis\"]:\n",
    "                augmented.append(new_row)\n",
    "\n",
    "    return pd.DataFrame(augmented).convert_dtypes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KjQLFQq8T7QR"
   },
   "source": [
    "Define augmentation strategies and their probability to be used for each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lSLivR29BGwt"
   },
   "outputs": [],
   "source": [
    "augmenters = [\n",
    "    (SynonymAug(aug_src='wordnet', aug_p=0.1), 0.8),   # Synonym replacement\n",
    "    (RandomWordAug(action='swap', aug_p=0.1), 0.1),    # Word swapping\n",
    "    (KeyboardAug(aug_char_p=0.1, aug_word_p=0.1), 0.3) # Typo simulation\n",
    "]\n",
    "\n",
    "df_aug = augment_df(df, augmenters)\n",
    "print(f\"Adding {len(df_aug)} new examples\")\n",
    "\n",
    "# Concatenate augmented dataset with original\n",
    "challenge_aug = Dataset.from_pandas(df_aug)\n",
    "# Uncomment this line to add augmented data to the training set\n",
    "# challenge_train = concatenate_datasets([challenge_train, challenge_aug]).shuffle()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZBykOxZDj7w3"
   },
   "source": [
    "### MNLI Finetuning\n",
    "\n",
    "Finetune the model on the MNLI dataset.\n",
    "\n",
    "**Architecture:**  \n",
    "- `FacebookAI/xlm-roberta-large` transformer from HuggingFace\n",
    "- Classification head with dropout (0.3)\n",
    "\n",
    "Train using the `Trainer` API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jyv-9MoVJWy6"
   },
   "source": [
    "#### Load the pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8o6XGXtgJhGB"
   },
   "outputs": [],
   "source": [
    "# Define constants\n",
    "PRETRAINED_MODEL_NAME = \"FacebookAI/xlm-roberta-large\"\n",
    "MODEL_DIR = ROOT_PATH / \"models\"\n",
    "MODEL_BASENAME = PRETRAINED_MODEL_NAME.rpartition('/')[2]\n",
    "\n",
    "# Pre-trained model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(PRETRAINED_MODEL_NAME, num_labels=3)\n",
    "model.classifier.dropout = torch.nn.Dropout(0.3)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)  # Move model to GPU if available\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    y_pred = np.argmax(eval_pred.predictions, axis=1)\n",
    "    return dict(accuracy=accuracy_score(eval_pred.label_ids, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)\n",
    "\n",
    "def tokenize_function(row):\n",
    "    return tokenizer(row['premise'], row['hypothesis'], padding='longest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set training hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnli_model_path = MODEL_DIR / f\"{MODEL_BASENAME}-mnli\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=mnli_model_path,\n",
    "    eval_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.1,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='accuracy',\n",
    "    report_to='none'  # Disabling wandb callbacks\n",
    ")\n",
    "\n",
    "mnli_train_tokenized = mnli_train.map(tokenize_function, batched=True)\n",
    "mnli_val_tokenized = mnli_val.map(tokenize_function, batched=True)\n",
    "\n",
    "mnli_trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=mnli_train_tokenized,\n",
    "    eval_dataset=mnli_val_tokenized,\n",
    "    processing_class=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnli_trainer.train()\n",
    "\n",
    "# Save the best model and the tokenizer to disk.\n",
    "mnli_trainer.save_model(mnli_model_path)\n",
    "tokenizer.save_pretrained(mnli_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XNLI Finetuning\n",
    "\n",
    "Finetune the model on the XNLI dataset.\n",
    "\n",
    "**Architecture:**  \n",
    "- `xml-roberta-large-mnli` fine-tuned model on MNLI dataset\n",
    "- Classification head with dropout (0.3)\n",
    "\n",
    "Train using `Trainer` with same hyperparameters and evaluate the model on the Challenge (train) dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnli_model = AutoModelForSequenceClassification.from_pretrained(mnli_model_path, num_labels=3)\n",
    "mnli_model.classifier.dropout = torch.nn.Dropout(0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set training hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xnli_model_path = MODEL_DIR / f\"{MODEL_BASENAME}-mnli-xnli\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=xnli_model_path,\n",
    "    eval_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.1,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='accuracy',\n",
    "    report_to='none'  # Disabling wandb callbacks\n",
    ")\n",
    "\n",
    "xnli_train_tokenized = xnli_train.map(tokenize_function, batched=True)\n",
    "challenge_tokenized = challenge_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "xnli_trainer = Trainer(\n",
    "    model=mnli_model,\n",
    "    args=training_args,\n",
    "    train_dataset=xnli_train_tokenized,\n",
    "    eval_dataset=challenge_tokenized,  # Use Challenge dataset set for evaluation\n",
    "    processing_class=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xnli_trainer.train()\n",
    "\n",
    "# Save the best model and the tokenizer to disk.\n",
    "xnli_trainer.save_model(xnli_model_path)\n",
    "tokenizer.save_pretrained(xnli_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upload to HuggingFace Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xnli_model = AutoModelForSequenceClassification.from_pretrained(xnli_model_path, num_labels=3)\n",
    "\n",
    "# Push the model to the hub\n",
    "xnli_model.push_to_hub(\n",
    "    f\"ajayat/{MODEL_BASENAME}-mnli-xnli\",\n",
    "    private=True,\n",
    "    use_auth_token=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wl19KSO2K5PP"
   },
   "source": [
    "### Evaluation & Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qZ94KVS5LHkg"
   },
   "source": [
    "Compute accuracy on the Challenge validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OvK1-BMbj7w4"
   },
   "outputs": [],
   "source": [
    "pred = xnli_trainer.predict(challenge_dataset)\n",
    "y_true = challenge_dataset['label']\n",
    "y_pred = np.argmax(pred.predictions, axis=-1)\n",
    "\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f\"\\nAccuracy: {accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mNVClqebLOXF"
   },
   "source": [
    "Display the confusion matrix using `seaborn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_d1sOBH1LDcQ"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(confusion_matrix(y_true, y_pred),\n",
    "            annot=True,\n",
    "            fmt='d',\n",
    "            cmap='Blues',\n",
    "            xticklabels=['Entailment', 'Neutral', 'Contradiction'],\n",
    "            yticklabels=['Entailment', 'Neutral', 'Contradiction'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jF49SrkCLWO7"
   },
   "outputs": [],
   "source": [
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=['Entailment', 'Neutral', 'Contradiction']))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
